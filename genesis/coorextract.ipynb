{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96734e09-c476-4f6a-9884-d5d5c28eaef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s-pose.pt to 'yolov8s-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.4M/22.4M [00:09<00:00, 2.53MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: D:\\FINAL PROJECT IG\\project_maal\\video\\jcr.mp4\n",
      "  Processed 100 frames...\n",
      "  Processed 200 frames...\n",
      "  Processed 300 frames...\n",
      "Finished processing D:\\FINAL PROJECT IG\\project_maal\\video\\jcr.mp4. Total frames saved: 308\n",
      "Pose coordinates saved to hoinu1.json\n",
      "\n",
      "Reference JSON file generation complete. Please use 'hoinu1.json' with your main cheating detection script.\n",
      "Remember to also update the 'left_cheating_landmarks.json' if needed, using a separate video.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Input video path and output JSON path\n",
    "video_path_to_process = r'D:\\FINAL PROJECT IG\\project_maal\\video\\jcr.mp4'\n",
    "output_json_path = 'hoinu1.json'\n",
    "\n",
    "# Load YOLOv8 pose model\n",
    "try:\n",
    "    model = YOLO(\"yolov8s-pose.pt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLOv8s-pose.pt model: {e}\")\n",
    "    print(\"Please ensure the model file is available and accessible in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Normalize landmarks relative to shoulders\n",
    "def normalize_landmarks(landmarks):\n",
    "    if len(landmarks) < 7:\n",
    "        print(\"Warning: Not enough landmarks (less than 7) for normalization. Skipping normalization.\")\n",
    "        return None\n",
    "    try:\n",
    "        if 5 not in range(len(landmarks)) or 6 not in range(len(landmarks)):\n",
    "             print(\"Warning: Shoulder landmarks (index 5 or 6) are missing in the detected set. Cannot normalize.\")\n",
    "             return None\n",
    "        left_shoulder = landmarks[5]\n",
    "        right_shoulder = landmarks[6]\n",
    "        if not all(k in left_shoulder for k in ['x', 'y']) or \\\n",
    "           not all(k in right_shoulder for k in ['x', 'y']):\n",
    "            print(\"Warning: Shoulder landmark data incomplete (missing 'x' or 'y' keys). Cannot normalize.\")\n",
    "            return None\n",
    "        center_x = (left_shoulder[\"x\"] + right_shoulder[\"x\"]) / 2\n",
    "        center_y = (left_shoulder[\"y\"] + right_shoulder[\"y\"]) / 2\n",
    "        scale = math.sqrt((left_shoulder[\"x\"] - right_shoulder[\"x\"]) ** 2 +\n",
    "                          (left_shoulder[\"y\"] - right_shoulder[\"y\"]) ** 2)\n",
    "        if scale == 0:\n",
    "            scale = 1e-6\n",
    "        normalized = []\n",
    "        for lm in landmarks:\n",
    "            if 'x' in lm and 'y' in lm:\n",
    "                norm_x = (lm[\"x\"] - center_x) / scale\n",
    "                norm_y = (lm[\"y\"] - center_y) / scale\n",
    "                normalized.append({\"x\": norm_x, \"y\": norm_y})\n",
    "            else:\n",
    "                print(f\"Warning: Skipping a landmark during normalization due to missing 'x' or 'y' data: {lm}\")\n",
    "                return None\n",
    "        return normalized\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during normalization: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process video and save normalized landmarks to JSON\n",
    "def process_video_for_landmarks(video_path, output_json_path, model):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "        return\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "    all_frames_data = []\n",
    "    frame_count = 0\n",
    "    print(f\"\\nProcessing video: {video_path}\")\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        results = model(frame, verbose=False)[0]\n",
    "        frame_data = {\n",
    "            \"frame\": frame_count,\n",
    "            \"landmarks\": []\n",
    "        }\n",
    "        if results.keypoints is not None and len(results.keypoints) > 0:\n",
    "            best_person_keypoints_data = None\n",
    "            max_keypoints = 0\n",
    "            for person_kp_obj in results.keypoints:\n",
    "                current_person_keypoints_data = person_kp_obj.data.cpu().numpy()\n",
    "                if current_person_keypoints_data.ndim == 3 and current_person_keypoints_data.shape[0] == 1:\n",
    "                    current_person_keypoints_data = current_person_keypoints_data[0]\n",
    "                if len(current_person_keypoints_data) > max_keypoints:\n",
    "                    max_keypoints = len(current_person_keypoints_data)\n",
    "                    best_person_keypoints_data = current_person_keypoints_data\n",
    "            if best_person_keypoints_data is not None:\n",
    "                height, width = frame.shape[:2]\n",
    "                current_frame_landmarks = []\n",
    "                for id, kp in enumerate(best_person_keypoints_data):\n",
    "                    x_norm = kp[0] / width\n",
    "                    y_norm = kp[1] / height\n",
    "                    current_frame_landmarks.append({\n",
    "                        \"id\": id,\n",
    "                        \"x\": x_norm,\n",
    "                        \"y\": y_norm,\n",
    "                        \"visibility\": kp[2]\n",
    "                    })\n",
    "                normalized_landmarks_for_frame = normalize_landmarks(current_frame_landmarks)\n",
    "                if normalized_landmarks_for_frame:\n",
    "                    frame_data[\"landmarks\"] = normalized_landmarks_for_frame\n",
    "                else:\n",
    "                    print(f\"Warning: Normalization failed for frame {frame_count} in {video_path}. Skipping landmarks for this frame.\")\n",
    "        all_frames_data.append(frame_data)\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"  Processed {frame_count} frames...\")\n",
    "    cap.release()\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(all_frames_data, f, indent=4)\n",
    "    print(f\"Finished processing {video_path}. Total frames saved: {len(all_frames_data)}\")\n",
    "    print(f\"Pose coordinates saved to {output_json_path}\")\n",
    "\n",
    "# Start processing\n",
    "process_video_for_landmarks(video_path_to_process, output_json_path, model)\n",
    "\n",
    "print(\"\\nReference JSON file generation complete. Please use 'hoinu1.json' with your main cheating detection script.\")\n",
    "print(\"Remember to also update the 'left_cheating_landmarks.json' if needed, using a separate video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7f1c26-7131-4a66-8dec-03a18037b72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: D:\\FINAL PROJECT IG\\project_maal\\video\\jcl.mp4\n",
      "  Processed 100 frames...\n",
      "  Processed 200 frames...\n",
      "  Processed 300 frames...\n",
      "Finished processing D:\\FINAL PROJECT IG\\project_maal\\video\\jcl.mp4. Total frames saved: 369\n",
      "Pose coordinates saved to hoinu2.json\n",
      "\n",
      "Reference JSON file generation complete. Please use 'hoinu2.json' with your main cheating detection script.\n",
      "Remember to update the path in your main script to load 'hoinu2.json' for left turn landmarks.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "video_path_to_process = r'D:\\FINAL PROJECT IG\\project_maal\\video\\jcl.mp4'  # Input video file path\n",
    "output_json_path = 'hoinu2.json'  # Output JSON file for normalized pose landmarks\n",
    "\n",
    "# --- Load YOLOv8 Pose Model ---\n",
    "try:\n",
    "    model = YOLO(\"yolov8s-pose.pt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLOv8s-pose.pt model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Normalize landmarks using shoulder distance (scale + center) ---\n",
    "def normalize_landmarks(landmarks):\n",
    "    if len(landmarks) < 7:\n",
    "        print(\"Warning: Not enough landmarks. Skipping normalization.\")\n",
    "        return None\n",
    "    try:\n",
    "        if 5 not in range(len(landmarks)) or 6 not in range(len(landmarks)):\n",
    "            print(\"Warning: Missing shoulder landmarks.\")\n",
    "            return None\n",
    "\n",
    "        left_shoulder = landmarks[5]\n",
    "        right_shoulder = landmarks[6]\n",
    "\n",
    "        if not all(k in left_shoulder for k in ['x', 'y']) or not all(k in right_shoulder for k in ['x', 'y']):\n",
    "            print(\"Warning: Shoulder data incomplete.\")\n",
    "            return None\n",
    "\n",
    "        center_x = (left_shoulder[\"x\"] + right_shoulder[\"x\"]) / 2\n",
    "        center_y = (left_shoulder[\"y\"] + right_shoulder[\"y\"]) / 2\n",
    "\n",
    "        scale = math.hypot(left_shoulder[\"x\"] - right_shoulder[\"x\"],\n",
    "                           left_shoulder[\"y\"] - right_shoulder[\"y\"])\n",
    "        if scale == 0:\n",
    "            scale = 1e-6  # avoid division by zero\n",
    "\n",
    "        normalized = []\n",
    "        for lm in landmarks:\n",
    "            if 'x' in lm and 'y' in lm:\n",
    "                norm_x = (lm[\"x\"] - center_x) / scale\n",
    "                norm_y = (lm[\"y\"] - center_y) / scale\n",
    "                normalized.append({\"x\": norm_x, \"y\": norm_y})\n",
    "            else:\n",
    "                print(f\"Warning: Landmark missing 'x' or 'y': {lm}\")\n",
    "                return None\n",
    "        return normalized\n",
    "    except Exception as e:\n",
    "        print(f\"Error during normalization: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Process video and extract pose landmarks for most visible person ---\n",
    "def process_video_for_landmarks(video_path, output_json_path, model):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video not found: {video_path}\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return\n",
    "\n",
    "    all_frames_data = []\n",
    "    frame_count = 0\n",
    "    print(f\"\\nProcessing video: {video_path}\")\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        results = model(frame, verbose=False)[0]\n",
    "\n",
    "        frame_data = {\n",
    "            \"frame\": frame_count,\n",
    "            \"landmarks\": []\n",
    "        }\n",
    "\n",
    "        # If keypoints are detected\n",
    "        if results.keypoints is not None and len(results.keypoints) > 0:\n",
    "            best_person_keypoints_data = None\n",
    "            max_keypoints = 0\n",
    "\n",
    "            # Choose person with most keypoints\n",
    "            for person_kp_obj in results.keypoints:\n",
    "                current_person_keypoints_data = person_kp_obj.data.cpu().numpy()\n",
    "                if current_person_keypoints_data.ndim == 3 and current_person_keypoints_data.shape[0] == 1:\n",
    "                    current_person_keypoints_data = current_person_keypoints_data[0]\n",
    "\n",
    "                if len(current_person_keypoints_data) > max_keypoints:\n",
    "                    max_keypoints = len(current_person_keypoints_data)\n",
    "                    best_person_keypoints_data = current_person_keypoints_data\n",
    "\n",
    "            # Convert and normalize best keypoints\n",
    "            if best_person_keypoints_data is not None:\n",
    "                height, width = frame.shape[:2]\n",
    "                current_frame_landmarks = []\n",
    "\n",
    "                for id, kp in enumerate(best_person_keypoints_data):\n",
    "                    x_norm = kp[0] / width\n",
    "                    y_norm = kp[1] / height\n",
    "                    current_frame_landmarks.append({\n",
    "                        \"id\": id,\n",
    "                        \"x\": x_norm,\n",
    "                        \"y\": y_norm,\n",
    "                        \"visibility\": kp[2]\n",
    "                    })\n",
    "\n",
    "                normalized_landmarks = normalize_landmarks(current_frame_landmarks)\n",
    "                if normalized_landmarks:\n",
    "                    frame_data[\"landmarks\"] = normalized_landmarks\n",
    "                else:\n",
    "                    print(f\"Warning: Normalization failed at frame {frame_count}\")\n",
    "\n",
    "        all_frames_data.append(frame_data)\n",
    "\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"  Processed {frame_count} frames...\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save all landmarks to JSON\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(all_frames_data, f, indent=4)\n",
    "\n",
    "    print(f\"Finished processing. Total frames: {len(all_frames_data)}\")\n",
    "    print(f\"Saved to {output_json_path}\")\n",
    "\n",
    "# --- Run the processor ---\n",
    "process_video_for_landmarks(video_path_to_process, output_json_path, model)\n",
    "\n",
    "print(\"\\nReference JSON file generation complete.\")\n",
    "print(\"Use 'hoinu2.json' in your main cheating detection script.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f13136-c847-4812-bc7a-33ac1d3114d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Finger Tracking)",
   "language": "python",
   "name": "finger_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
